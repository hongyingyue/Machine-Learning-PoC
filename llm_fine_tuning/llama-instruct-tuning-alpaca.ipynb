{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":120005,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":100936,"modelId":121027}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install bitsandbytes --quiet\n!pip install transformers --quiet\n!pip install peft --quiet\n!pip install accelerate --quiet\n!pip install trl --quiet\n!pip install datasets --quiet","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-17T21:19:10.352402Z","iopub.execute_input":"2025-05-17T21:19:10.352693Z","iopub.status.idle":"2025-05-17T21:20:46.665131Z","shell.execute_reply.started":"2025-05-17T21:19:10.352672Z","shell.execute_reply":"2025-05-17T21:20:46.664076Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.0/348.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport torch\nfrom datasets import load_dataset\nfrom trl import SFTTrainer\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\nfrom peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T21:20:46.666888Z","iopub.execute_input":"2025-05-17T21:20:46.667148Z","iopub.status.idle":"2025-05-17T21:21:18.147015Z","shell.execute_reply.started":"2025-05-17T21:20:46.667124Z","shell.execute_reply":"2025-05-17T21:21:18.146450Z"}},"outputs":[{"name":"stderr","text":"2025-05-17 21:21:01.450910: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747516861.668763      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747516861.730283      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"dataset = load_dataset(\"tatsu-lab/alpaca\")[\"train\"]\n\ndataset[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T21:21:18.147720Z","iopub.execute_input":"2025-05-17T21:21:18.148284Z","iopub.status.idle":"2025-05-17T21:21:20.322934Z","shell.execute_reply.started":"2025-05-17T21:21:18.148264Z","shell.execute_reply":"2025-05-17T21:21:20.322343Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/7.47k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e9a958e7bbe4ae7ad8b6e677d2f3050"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00001-a09b74b3ef9c3b(…):   0%|          | 0.00/24.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a0556822a9f4a39adef189d5ff4af05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/52002 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5dc3cd7e1bb340d0a02b796783ceec37"}},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"{'instruction': 'Give three tips for staying healthy.',\n 'input': '',\n 'output': '1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.',\n 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGive three tips for staying healthy.\\n\\n### Response:\\n1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.'}"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"model_name_or_path = \"/kaggle/input/llama-3.2/transformers/3b-instruct/1\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)\ntokenizer.padding_side = \"right\"\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.add_eos_token = True\n\nprint(tokenizer.bos_token, tokenizer.eos_token)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T21:21:20.323671Z","iopub.execute_input":"2025-05-17T21:21:20.323964Z","iopub.status.idle":"2025-05-17T21:21:20.902958Z","shell.execute_reply.started":"2025-05-17T21:21:20.323921Z","shell.execute_reply":"2025-05-17T21:21:20.902157Z"}},"outputs":[{"name":"stdout","text":"<|begin_of_text|> <|eot_id|>\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"def format_chat_template(row):\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": row[\"instruction\"]},\n        {\"role\": \"assistant\", \"content\": row[\"output\"]}\n    ]\n    \n    chat = tokenizer.apply_chat_template(messages, tokenize=False)\n    return {\"text\": chat}\n\n\nformat_chat_template(dataset[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T21:21:20.905083Z","iopub.execute_input":"2025-05-17T21:21:20.905315Z","iopub.status.idle":"2025-05-17T21:21:20.916349Z","shell.execute_reply.started":"2025-05-17T21:21:20.905297Z","shell.execute_reply":"2025-05-17T21:21:20.915614Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"{'text': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nGive three tips for staying healthy.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'}"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"dataset = dataset.shuffle(seed=42).select(range(11000))\n\nprocessed_dataset = dataset.map(\n    format_chat_template,\n    num_proc= os.cpu_count(),\n)\n\ndataset = processed_dataset.train_test_split(test_size=0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T21:21:20.916941Z","iopub.execute_input":"2025-05-17T21:21:20.917153Z","iopub.status.idle":"2025-05-17T21:21:22.360738Z","shell.execute_reply.started":"2025-05-17T21:21:20.917136Z","shell.execute_reply":"2025-05-17T21:21:22.360076Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/11000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bae3137775e747f6bdecabe327f556ee"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"compute_dtype = torch.bfloat16\nbnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=compute_dtype,\n        bnb_4bit_use_double_quant=True\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name_or_path,\n    trust_remote_code=True,\n    quantization_config=bnb_config,\n    device_map='auto',\n)\nmodel.config.use_cache = False\nmodel.gradient_checkpointing_enable()\n\nmodel = prepare_model_for_kbit_training(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T21:21:22.361651Z","iopub.execute_input":"2025-05-17T21:21:22.361902Z","iopub.status.idle":"2025-05-17T21:21:55.668826Z","shell.execute_reply.started":"2025-05-17T21:21:22.361857Z","shell.execute_reply":"2025-05-17T21:21:55.668016Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04893f8d0bd24e9e8b9d758e21973c41"}},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"peft_config = LoraConfig(\n    lora_alpha=64,\n    lora_dropout=0.05,\n    r=4,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=\"all-linear\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T21:21:55.669769Z","iopub.execute_input":"2025-05-17T21:21:55.670237Z","iopub.status.idle":"2025-05-17T21:21:55.674452Z","shell.execute_reply.started":"2025-05-17T21:21:55.670209Z","shell.execute_reply":"2025-05-17T21:21:55.673694Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n        output_dir=\"./llama3_sft/\",\n        do_eval=True,\n        # eval_strategy=\"epoch\",\n        optim=\"paged_adamw_8bit\",\n        per_device_train_batch_size=8,\n        gradient_accumulation_steps=2,\n        per_device_eval_batch_size=8,\n        logging_steps=10,\n        learning_rate=8e-6,\n        num_train_epochs=2,\n        warmup_steps=3,\n        lr_scheduler_type=\"linear\",\n        max_steps=200,  # demo temp\n        # fp16=True, \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T21:21:55.675100Z","iopub.execute_input":"2025-05-17T21:21:55.675272Z","iopub.status.idle":"2025-05-17T21:21:56.027180Z","shell.execute_reply.started":"2025-05-17T21:21:55.675257Z","shell.execute_reply":"2025-05-17T21:21:56.026571Z"}},"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"trainer = SFTTrainer(\n        model=model,\n        train_dataset=dataset['train'],\n        eval_dataset=dataset['test'],\n        peft_config=peft_config,\n        args=training_arguments,\n)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T21:21:56.027977Z","iopub.execute_input":"2025-05-17T21:21:56.028216Z","iopub.status.idle":"2025-05-17T22:27:03.427772Z","shell.execute_reply.started":"2025-05-17T21:21:56.028192Z","shell.execute_reply":"2025-05-17T22:27:03.426942Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Converting train dataset to ChatML:   0%|          | 0/9900 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1f477532f7143dfa82f3f34c0cb7d03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Adding EOS to train dataset:   0%|          | 0/9900 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9ec700782cc4f22b5cb08b2892051eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset:   0%|          | 0/9900 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac239f54b2d5491691f8f6d5c4e98592"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating train dataset:   0%|          | 0/9900 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04c8755bd28546ec8be4fdca3c8baaa8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Converting eval dataset to ChatML:   0%|          | 0/1100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32aec4fc13a44105922731a9114287c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Adding EOS to eval dataset:   0%|          | 0/1100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97c0c57cc4b74a2390a40830e0b9e6b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing eval dataset:   0%|          | 0/1100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d421725645b4b279971dbb6637fb128"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating eval dataset:   0%|          | 0/1100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a955786081264f6eb2d9da26c7c67a2b"}},"metadata":{}},{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [200/200 1:04:48, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>2.871600</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>2.576800</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>2.196500</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>2.024300</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.784300</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.618500</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>1.524300</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.448200</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>1.450400</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.406500</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>1.392100</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>1.427900</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>1.417400</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>1.402700</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.402300</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>1.356500</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>1.375300</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>1.382400</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>1.426700</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.352500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=200, training_loss=1.6418570566177368, metrics={'train_runtime': 3898.6987, 'train_samples_per_second': 0.821, 'train_steps_per_second': 0.051, 'total_flos': 9810597599281152.0, 'train_loss': 1.6418570566177368})"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"sample = dataset[\"test\"][0]\nprint(sample)\n\n\ndef format_chat_template(row):\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": row[\"instruction\"]}\n    ]    \n    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    return prompt\n\nprompt = format_chat_template(sample)\nprompt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T22:27:03.428724Z","iopub.execute_input":"2025-05-17T22:27:03.428978Z","iopub.status.idle":"2025-05-17T22:27:03.436206Z","shell.execute_reply.started":"2025-05-17T22:27:03.428960Z","shell.execute_reply":"2025-05-17T22:27:03.435494Z"}},"outputs":[{"name":"stdout","text":"{'instruction': 'Provide the necessary materials for the given project.', 'input': 'Build a birdhouse', 'output': 'Materials Needed for Building a Birdhouse:\\n-Pieces of wood for the base, walls and roof of the birdhouse \\n-Saw \\n-Screws \\n-Screwdriver \\n-Nails \\n-Hammer \\n-Paint\\n-Paintbrushes \\n-Drill and bits \\n-Gravel (optional)', 'text': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nProvide the necessary materials for the given project.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nMaterials Needed for Building a Birdhouse:\\n-Pieces of wood for the base, walls and roof of the birdhouse \\n-Saw \\n-Screws \\n-Screwdriver \\n-Nails \\n-Hammer \\n-Paint\\n-Paintbrushes \\n-Drill and bits \\n-Gravel (optional)<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'}\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nProvide the necessary materials for the given project.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# sft_model = AutoModelForCausalLM.from_pretrained(\"./llama3_sft/\")\n\nsft_model = AutoModelForCausalLM.from_pretrained(\"/kaggle/input/llama-3.2/transformers/3b-instruct/1\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T22:27:03.436908Z","iopub.execute_input":"2025-05-17T22:27:03.437240Z","iopub.status.idle":"2025-05-17T22:27:10.663130Z","shell.execute_reply.started":"2025-05-17T22:27:03.437221Z","shell.execute_reply":"2025-05-17T22:27:10.662463Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"668ac75fc0574f1f82a18ceb7e774b35"}},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n\noutput = model.generate(\n    **inputs,\n    max_new_tokens=100,\n    do_sample=True,\n    temperature=0.7,\n    pad_token_id=tokenizer.eos_token_id,\n)\n\ngenerated_text = tokenizer.decode(output[0], skip_special_tokens=False)\nprint(generated_text[len(prompt):].split(\"<|eot_id|>\")[0]) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T22:27:10.663898Z","iopub.execute_input":"2025-05-17T22:27:10.664164Z","iopub.status.idle":"2025-05-17T22:27:17.590203Z","shell.execute_reply.started":"2025-05-17T22:27:10.664145Z","shell.execute_reply":"2025-05-17T22:27:17.589393Z"}},"outputs":[{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"The materials needed for this project are:\n- A piece of wood\n- A drill\n- A hammer\n- A saw\n- A sandpaper\n- Paint\n- Paintbrushes\n- A paint tray\n","output_type":"stream"}],"execution_count":13}]}